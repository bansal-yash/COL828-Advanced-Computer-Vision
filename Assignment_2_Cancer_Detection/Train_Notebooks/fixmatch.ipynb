{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:09.666274Z",
     "iopub.status.busy": "2025-11-21T12:29:09.665402Z",
     "iopub.status.idle": "2025-11-21T12:29:22.397666Z",
     "shell.execute_reply": "2025-11-21T12:29:22.396811Z",
     "shell.execute_reply.started": "2025-11-21T12:29:09.666243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, ConcatDataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.399800Z",
     "iopub.status.busy": "2025-11-21T12:29:22.399249Z",
     "iopub.status.idle": "2025-11-21T12:29:22.403553Z",
     "shell.execute_reply": "2025-11-21T12:29:22.402884Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.399776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/kaggle/input/mammography\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.404689Z",
     "iopub.status.busy": "2025-11-21T12:29:22.404432Z",
     "iopub.status.idle": "2025-11-21T12:29:22.428572Z",
     "shell.execute_reply": "2025-11-21T12:29:22.427818Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.404671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, train=False, strong_aug=True):\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.train = train\n",
    "        self.strong_aug = strong_aug\n",
    "        self.label_map = {\"BENIGN\": 0, \"MALIGNANT\": 1}\n",
    "        self.data[\"label\"] = self.data[\"pathology\"].map(self.label_map)\n",
    "        for col in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]:\n",
    "            if col in self.data.columns:\n",
    "                self.data[col] = self.data[col].fillna(0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        original_width, original_height = image.size\n",
    "        \n",
    "        target_height, target_width = 981, 800\n",
    "        \n",
    "        image = image.resize((target_width, target_height), Image.BILINEAR)\n",
    "        scale_x = target_width / original_width\n",
    "        scale_y = target_height / original_height\n",
    "        \n",
    "        scaled_xmin = row[\"xmin\"] * scale_x\n",
    "        scaled_ymin = row[\"ymin\"] * scale_y\n",
    "        scaled_xmax = row[\"xmax\"] * scale_x\n",
    "        scaled_ymax = row[\"ymax\"] * scale_y\n",
    "        \n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        \n",
    "        if self.train:\n",
    "            image, scaled_xmin, scaled_ymin, scaled_xmax, scaled_ymax = self._apply_augmentations(\n",
    "                image, scaled_xmin, scaled_ymin, scaled_xmax, scaled_ymax, target_width, target_height\n",
    "            )\n",
    "        \n",
    "        if label == 1:\n",
    "            bbox_xywh = torch.tensor(\n",
    "                [\n",
    "                    scaled_xmin,\n",
    "                    scaled_ymin,\n",
    "                    scaled_xmax - scaled_xmin,\n",
    "                    scaled_ymax - scaled_ymin,\n",
    "                ],\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "            annotations = {\n",
    "                \"image_id\": 0,\n",
    "                \"annotations\": [\n",
    "                    {\n",
    "                        \"bbox\": bbox_xywh,\n",
    "                        \"area\": float(bbox_xywh[2] * bbox_xywh[3]),\n",
    "                        \"category_id\": 0,\n",
    "                        \"iscrowd\": 0,\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        else:\n",
    "            annotations = {\"image_id\": 0, \"annotations\": []}\n",
    "        \n",
    "        return image, annotations\n",
    "    \n",
    "    def _apply_augmentations(self, image, xmin, ymin, xmax, ymax, img_width, img_height):\n",
    "        if np.random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            xmin_new = img_width - xmax\n",
    "            xmax_new = img_width - xmin\n",
    "            xmin, xmax = xmin_new, xmax_new\n",
    "            \n",
    "        if np.random.random() > 0.5:\n",
    "            scale_factor = np.random.uniform(0.95, 1.05)\n",
    "            new_width = int(img_width * scale_factor)\n",
    "            new_height = int(img_height * scale_factor)\n",
    "            \n",
    "            image = TF.resize(image, (new_height, new_width), interpolation=TF.InterpolationMode.BILINEAR)\n",
    "            \n",
    "            if scale_factor > 1:\n",
    "                left = (new_width - img_width) // 2\n",
    "                top = (new_height - img_height) // 2\n",
    "                image = TF.crop(image, top, left, img_height, img_width)\n",
    "                \n",
    "                xmin = xmin * scale_factor - left\n",
    "                ymin = ymin * scale_factor - top\n",
    "                xmax = xmax * scale_factor - left\n",
    "                ymax = ymax * scale_factor - top\n",
    "            else:\n",
    "                padding_left = (img_width - new_width) // 2\n",
    "                padding_top = (img_height - new_height) // 2\n",
    "                padding_right = img_width - new_width - padding_left\n",
    "                padding_bottom = img_height - new_height - padding_top\n",
    "                \n",
    "                image = TF.pad(image, (padding_left, padding_top, padding_right, padding_bottom), fill=0)\n",
    "                \n",
    "                xmin = xmin * scale_factor + padding_left\n",
    "                ymin = ymin * scale_factor + padding_top\n",
    "                xmax = xmax * scale_factor + padding_left\n",
    "                ymax = ymax * scale_factor + padding_top\n",
    "            \n",
    "            xmin = max(0, min(xmin, img_width))\n",
    "            ymin = max(0, min(ymin, img_height))\n",
    "            xmax = max(0, min(xmax, img_width))\n",
    "            ymax = max(0, min(ymax, img_height))\n",
    "\n",
    "        if self.strong_aug:\n",
    "            if np.random.random() > 0.5:\n",
    "                angle = np.random.uniform(-5, 5)\n",
    "                image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR, fill=0)\n",
    "                \n",
    "                center_x, center_y = img_width / 2, img_height / 2\n",
    "                angle_rad = -angle * np.pi / 180\n",
    "                \n",
    "                corners = [\n",
    "                    (xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)\n",
    "                ]\n",
    "                rotated_corners = []\n",
    "                for x, y in corners:\n",
    "                    x_temp = x - center_x\n",
    "                    y_temp = y - center_y\n",
    "                    x_new = x_temp * np.cos(angle_rad) - y_temp * np.sin(angle_rad)\n",
    "                    y_new = x_temp * np.sin(angle_rad) + y_temp * np.cos(angle_rad)\n",
    "                    rotated_corners.append((x_new + center_x, y_new + center_y))\n",
    "                \n",
    "                xs = [c[0] for c in rotated_corners]\n",
    "                ys = [c[1] for c in rotated_corners]\n",
    "                xmin, xmax = min(xs), max(xs)\n",
    "                ymin, ymax = min(ys), max(ys)\n",
    "                \n",
    "                xmin = max(0, min(xmin, img_width))\n",
    "                ymin = max(0, min(ymin, img_height))\n",
    "                xmax = max(0, min(xmax, img_width))\n",
    "                ymax = max(0, min(ymax, img_height))\n",
    "        \n",
    "            if np.random.random() > 0.5:\n",
    "                brightness_factor = np.random.uniform(0.85, 1.15)\n",
    "                image = TF.adjust_brightness(image, brightness_factor)\n",
    "            \n",
    "            if np.random.random() > 0.5:\n",
    "                contrast_factor = np.random.uniform(0.85, 1.15)\n",
    "                image = TF.adjust_contrast(image, contrast_factor)\n",
    "            \n",
    "            if np.random.random() > 0.5:\n",
    "                gamma = np.random.uniform(0.9, 1.1)\n",
    "                image = TF.adjust_gamma(image, gamma)\n",
    "        \n",
    "        return image, xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.429594Z",
     "iopub.status.busy": "2025-11-21T12:29:22.429352Z",
     "iopub.status.idle": "2025-11-21T12:29:22.446977Z",
     "shell.execute_reply": "2025-11-21T12:29:22.446286Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.429567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def grounding_dino_collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "    return list(images), list(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.449002Z",
     "iopub.status.busy": "2025-11-21T12:29:22.448685Z",
     "iopub.status.idle": "2025-11-21T12:29:22.461949Z",
     "shell.execute_reply": "2025-11-21T12:29:22.461279Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.448983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_train_dataloaders(train_name_a, train_name_b, labeled_fraction=0.1):\n",
    "    train_csv_a = f\"{DATASET_ROOT}/dataset_{train_name_a}/train/train.csv\"\n",
    "    train_img_dir_a = f\"{DATASET_ROOT}/dataset_{train_name_a}/train\"\n",
    "    train_full_df_a = pd.read_csv(train_csv_a)\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx_a, val_idx_a = next(splitter.split(train_full_df_a, train_full_df_a[\"pathology\"]))\n",
    "    train_df_a = train_full_df_a.iloc[train_idx_a].reset_index(drop=True)\n",
    "    val_df_a = train_full_df_a.iloc[val_idx_a].reset_index(drop=True)\n",
    "    \n",
    "    train_csv_b = f\"{DATASET_ROOT}/dataset_{train_name_b}/train/train.csv\"\n",
    "    train_img_dir_b = f\"{DATASET_ROOT}/dataset_{train_name_b}/train\"\n",
    "    train_full_df_b = pd.read_csv(train_csv_b)\n",
    "    \n",
    "    train_idx_b, val_idx_b = next(splitter.split(train_full_df_b, train_full_df_b[\"pathology\"]))\n",
    "    train_df_b = train_full_df_b.iloc[train_idx_b].reset_index(drop=True)\n",
    "    val_df_b = train_full_df_b.iloc[val_idx_b].reset_index(drop=True)\n",
    "    \n",
    "    splitter_b = StratifiedShuffleSplit(n_splits=1, test_size=1-labeled_fraction, random_state=42)\n",
    "    labeled_idx_b, unlabeled_idx_b = next(splitter_b.split(train_df_b, train_df_b[\"pathology\"]))\n",
    "    train_df_b_labeled = train_df_b.iloc[labeled_idx_b].reset_index(drop=True)\n",
    "    train_df_b_unlabeled = train_df_b.iloc[unlabeled_idx_b].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset_a = MammographyDataset(train_df_a, train_img_dir_a)\n",
    "    train_dataset_b_labeled = MammographyDataset(train_df_b_labeled, train_img_dir_b)\n",
    "    supervised_train_dataset = ConcatDataset([train_dataset_a, train_dataset_b_labeled])\n",
    "    \n",
    "    unsupervised_train_dataset = MammographyDataset(train_df_b_unlabeled, train_img_dir_b, train=True, strong_aug=False)\n",
    "    \n",
    "    val_dataset_a = MammographyDataset(val_df_a, train_img_dir_a)\n",
    "    val_dataset_b = MammographyDataset(val_df_b, train_img_dir_b)\n",
    "    \n",
    "    combined_df = pd.concat([train_df_a, train_df_b_labeled], ignore_index=True)\n",
    "    class_counts = combined_df[\"pathology\"].value_counts().to_dict()\n",
    "    weights = combined_df[\"pathology\"].apply(lambda x: 1.0 / class_counts[x]).values\n",
    "    sampler = WeightedRandomSampler(weights=weights, num_samples=len(combined_df), replacement=True)\n",
    "    \n",
    "    supervised_train_loader = DataLoader(\n",
    "        supervised_train_dataset,\n",
    "        batch_size=2,\n",
    "        num_workers=4,\n",
    "        collate_fn=grounding_dino_collate_fn,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    \n",
    "    unsupervised_train_loader = DataLoader(\n",
    "        unsupervised_train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=grounding_dino_collate_fn,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        ConcatDataset([val_dataset_a, val_dataset_b]),\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=grounding_dino_collate_fn,\n",
    "    )\n",
    "    \n",
    "    return supervised_train_loader, unsupervised_train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.462855Z",
     "iopub.status.busy": "2025-11-21T12:29:22.462650Z",
     "iopub.status.idle": "2025-11-21T12:29:22.478112Z",
     "shell.execute_reply": "2025-11-21T12:29:22.477291Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.462838Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_test_dataloaders():\n",
    "    datasets = [\"A\", \"B\", \"C\"]\n",
    "    test_loaders = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        test_csv = f\"{DATASET_ROOT}/dataset_{dataset}/test/test.csv\"\n",
    "        test_img_dir = f\"{DATASET_ROOT}/dataset_{dataset}/test\"\n",
    "\n",
    "        test_df = pd.read_csv(test_csv)\n",
    "        test_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in test_df.columns]\n",
    "\n",
    "        test_dataset = MammographyDataset(test_df, test_img_dir)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            collate_fn=grounding_dino_collate_fn,\n",
    "        )\n",
    "        test_loaders.append(test_loader)\n",
    "\n",
    "    return tuple(test_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.479154Z",
     "iopub.status.busy": "2025-11-21T12:29:22.478870Z",
     "iopub.status.idle": "2025-11-21T12:29:22.494526Z",
     "shell.execute_reply": "2025-11-21T12:29:22.493702Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.479134Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    train_name,\n",
    "    num_epochs,\n",
    "    train_losses,\n",
    "    val_losses,\n",
    "    train_map50_scores,\n",
    "    val_map50_scores,\n",
    "    train_map75_scores,\n",
    "    val_map75_scores,\n",
    "    train_map_scores,\n",
    "    val_map_scores,\n",
    "    sup_train_losses,\n",
    "    unsup_train_losses,\n",
    "):\n",
    "    epochs_range = range(1, num_epochs + 1)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    axes[0, 0].plot(\n",
    "        epochs_range,\n",
    "        sup_train_losses,\n",
    "        \"b-o\",\n",
    "        label=\"Supervised Loss\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[0, 0].plot(\n",
    "        epochs_range,\n",
    "        unsup_train_losses,\n",
    "        \"g-^\",\n",
    "        label=\"Unsupervised Loss\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[0, 0].plot(\n",
    "        epochs_range,\n",
    "        val_losses,\n",
    "        \"r-s\",\n",
    "        label=\"Validation Loss\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[0, 0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "    axes[0, 0].set_ylabel(\"Loss (per sample)\", fontsize=12)\n",
    "    axes[0, 0].set_title(\n",
    "        f\"Training Losses (Sup/Unsup) and Validation Loss - {train_name}\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(\n",
    "        epochs_range,\n",
    "        train_map50_scores,\n",
    "        \"b-o\",\n",
    "        label=\"Training mAP@50\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[0, 1].plot(\n",
    "        epochs_range,\n",
    "        val_map50_scores,\n",
    "        \"r-s\",\n",
    "        label=\"Validation mAP@50\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[0, 1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "    axes[0, 1].set_ylabel(\"mAP@50\", fontsize=12)\n",
    "    axes[0, 1].set_title(\n",
    "        f\"mAP@50 (IoU=0.5) - {train_name}\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, 1].legend(fontsize=11)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(\n",
    "        epochs_range,\n",
    "        train_map75_scores,\n",
    "        \"b-o\",\n",
    "        label=\"Training mAP@75\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[1, 0].plot(\n",
    "        epochs_range,\n",
    "        val_map75_scores,\n",
    "        \"r-s\",\n",
    "        label=\"Validation mAP@75\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[1, 0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "    axes[1, 0].set_ylabel(\"mAP@75\", fontsize=12)\n",
    "    axes[1, 0].set_title(\n",
    "        f\"mAP@75 (IoU=0.75) - {train_name}\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(\n",
    "        epochs_range,\n",
    "        train_map_scores,\n",
    "        \"b-o\",\n",
    "        label=\"Training mAP\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[1, 1].plot(\n",
    "        epochs_range,\n",
    "        val_map_scores,\n",
    "        \"r-s\",\n",
    "        label=\"Validation mAP\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "    axes[1, 1].set_ylabel(\"mAP\", fontsize=12)\n",
    "    axes[1, 1].set_title(\n",
    "        f\"Mean Average Precision (mAP) - {train_name}\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"training_metrics_{train_name}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Changed: Added sup and unsup losses to CSV\n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": list(epochs_range),\n",
    "            \"train_loss\": train_losses,\n",
    "            \"sup_train_loss\": sup_train_losses,\n",
    "            \"unsup_train_loss\": unsup_train_losses,\n",
    "            \"val_loss\": val_losses,\n",
    "            \"train_map50\": train_map50_scores,\n",
    "            \"val_map50\": val_map50_scores,\n",
    "            \"train_map75\": train_map75_scores,\n",
    "            \"val_map75\": val_map75_scores,\n",
    "            \"train_map\": train_map_scores,\n",
    "            \"val_map\": val_map_scores,\n",
    "        }\n",
    "    )\n",
    "    metrics_df.to_csv(f\"training_metrics_{train_name}.csv\", index=False)\n",
    "    print(f\"\\nâœ“ Metrics saved to training_metrics_{train_name}.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"FINAL TRAINING SUMMARY - {train_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Final Training Loss (Total): {train_losses[-1]:.6f}\")\n",
    "    print(f\"Final Supervised Loss: {sup_train_losses[-1]:.6f}\")\n",
    "    print(f\"Final Unsupervised Loss: {unsup_train_losses[-1]:.6f}\")\n",
    "    print(f\"Final Validation Loss: {val_losses[-1]:.6f}\")\n",
    "    print(f\"Final Training mAP@50: {train_map50_scores[-1]:.4f}\")\n",
    "    print(f\"Final Validation mAP@50: {val_map50_scores[-1]:.4f}\")\n",
    "    print(f\"Final Training mAP@75: {train_map75_scores[-1]:.4f}\")\n",
    "    print(f\"Final Validation mAP@75: {val_map75_scores[-1]:.4f}\")\n",
    "    print(f\"Final Training mAP: {train_map_scores[-1]:.4f}\")\n",
    "    print(f\"Final Validation mAP: {val_map_scores[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.495631Z",
     "iopub.status.busy": "2025-11-21T12:29:22.495305Z",
     "iopub.status.idle": "2025-11-21T12:29:22.511217Z",
     "shell.execute_reply": "2025-11-21T12:29:22.510317Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.495612Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CoOpContext(nn.Module):\n",
    "    def __init__(self, device, processor, model, initial_prompt):\n",
    "        super().__init__()\n",
    "\n",
    "        tokens = processor.tokenizer(\n",
    "            initial_prompt, return_tensors=\"pt\", padding=False\n",
    "        ).to(device)\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embeds = model.model.text_backbone.embeddings.word_embeddings(\n",
    "                input_ids\n",
    "            )\n",
    "\n",
    "        full_embeds = text_embeds[0, :, :]\n",
    "\n",
    "        self.all_embeddings = nn.Parameter(full_embeds.clone())\n",
    "        self.initial_token_ids = input_ids[0].clone()\n",
    "\n",
    "        seq_len = full_embeds.shape[0]\n",
    "        self.trainable_mask = torch.ones(seq_len, dtype=torch.bool)\n",
    "        self.trainable_mask[0] = False\n",
    "        self.trainable_mask[-2] = False\n",
    "        self.trainable_mask[-1] = False\n",
    "\n",
    "        def freeze_hook(grad):\n",
    "            mask = self.trainable_mask.unsqueeze(-1).to(grad.device)\n",
    "            return grad * mask\n",
    "\n",
    "        self.hook_handle = self.all_embeddings.register_hook(freeze_hook)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.all_embeddings\n",
    "\n",
    "    def parameters(self, recurse=True):\n",
    "        return super().parameters(recurse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.512935Z",
     "iopub.status.busy": "2025-11-21T12:29:22.512113Z",
     "iopub.status.idle": "2025-11-21T12:29:22.524862Z",
     "shell.execute_reply": "2025-11-21T12:29:22.524096Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.512907Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def move_labels_to_device(labels, device):\n",
    "    new_labels = []\n",
    "    for lbl in labels:\n",
    "        new_lbl = {}\n",
    "        for k, v in lbl.items():\n",
    "            if torch.is_tensor(v):\n",
    "                new_lbl[k] = v.to(device)\n",
    "            else:\n",
    "                new_lbl[k] = v\n",
    "        new_labels.append(new_lbl)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.526017Z",
     "iopub.status.busy": "2025-11-21T12:29:22.525711Z",
     "iopub.status.idle": "2025-11-21T12:29:22.545057Z",
     "shell.execute_reply": "2025-11-21T12:29:22.544317Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.525993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform_boxes(boxes, transforms_applied, img_shape):\n",
    "    if len(boxes) == 0:\n",
    "        return boxes\n",
    "    \n",
    "    h, w = img_shape\n",
    "    \n",
    "    for transform_name, params in transforms_applied:\n",
    "        if transform_name == 'rotate':\n",
    "            angle = params\n",
    "            angle_rad = np.radians(angle)\n",
    "            \n",
    "            cx, cy = w / 2, h / 2\n",
    "            \n",
    "            boxes_transformed = []\n",
    "            \n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                \n",
    "                corners = torch.tensor([\n",
    "                    [x1, y1],\n",
    "                    [x2, y1],\n",
    "                    [x2, y2],\n",
    "                    [x1, y2]\n",
    "                ], device=box.device, dtype=box.dtype)\n",
    "                \n",
    "                corners[:, 0] -= cx\n",
    "                corners[:, 1] -= cy\n",
    "                \n",
    "                cos_a = np.cos(angle_rad)\n",
    "                sin_a = np.sin(angle_rad)\n",
    "                \n",
    "                rotated_corners = corners.clone()\n",
    "                rotated_corners[:, 0] = corners[:, 0] * cos_a - corners[:, 1] * sin_a\n",
    "                rotated_corners[:, 1] = corners[:, 0] * sin_a + corners[:, 1] * cos_a\n",
    "                \n",
    "                rotated_corners[:, 0] += cx\n",
    "                rotated_corners[:, 1] += cy\n",
    "                \n",
    "                x_min = rotated_corners[:, 0].min()\n",
    "                y_min = rotated_corners[:, 1].min()\n",
    "                x_max = rotated_corners[:, 0].max()\n",
    "                y_max = rotated_corners[:, 1].max()\n",
    "                \n",
    "                x_min = torch.clamp(x_min, 0, w)\n",
    "                y_min = torch.clamp(y_min, 0, h)\n",
    "                x_max = torch.clamp(x_max, 0, w)\n",
    "                y_max = torch.clamp(y_max, 0, h)\n",
    "                \n",
    "                boxes_transformed.append(torch.stack([x_min, y_min, x_max, y_max]))\n",
    "            \n",
    "            boxes = torch.stack(boxes_transformed)\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def apply_strong_augmentation(image, boxes=None):\n",
    "    transforms_applied = []\n",
    "    \n",
    "    if np.random.random() > 0.5:\n",
    "        brightness_factor = np.random.uniform(0.7, 1.3)\n",
    "        image = TF.adjust_brightness(image, brightness_factor)\n",
    "    \n",
    "    if np.random.random() > 0.5:\n",
    "        contrast_factor = np.random.uniform(0.7, 1.3)\n",
    "        image = TF.adjust_contrast(image, contrast_factor)\n",
    "    \n",
    "    if np.random.random() > 0.5:\n",
    "        gamma = np.random.uniform(0.8, 1.2)\n",
    "        image = TF.adjust_gamma(image, gamma)\n",
    "    \n",
    "    angle = 0\n",
    "    if np.random.random() > 0.5:\n",
    "        angle = np.random.uniform(-10, 10)\n",
    "        image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR, fill=0)\n",
    "        transforms_applied.append(('rotate', angle))\n",
    "    \n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        w, h = image.size\n",
    "        img_shape = (h, w)\n",
    "        boxes = transform_boxes(boxes, transforms_applied, img_shape)\n",
    "        return image, boxes\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.546327Z",
     "iopub.status.busy": "2025-11-21T12:29:22.546018Z",
     "iopub.status.idle": "2025-11-21T12:29:22.586554Z",
     "shell.execute_reply": "2025-11-21T12:29:22.585824Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.546299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_dataset(model, processor, sup_train_name, unsup_train_name, num_epochs):\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"#  SEMI-SUPERVISED TRAINING\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "\n",
    "    sup_train_loader, unsup_train_loader, val_loader = create_train_dataloaders(sup_train_name, unsup_train_name)\n",
    "\n",
    "    initial_prompt = \"malignant tumor cancer.\"\n",
    "    conf_threshold = 0.1\n",
    "    lambda_u = 1\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Configuration\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Supervised Dataset   : {sup_train_name}\")\n",
    "    print(f\"  Unsupervised Dataset : {unsup_train_name}\")\n",
    "    print(f\"  Epochs               : {num_epochs}\")\n",
    "    print(f\"  Base Prompt          : '{initial_prompt}'\")\n",
    "    print(f\"  Lambda_u (target)    : {lambda_u}\")\n",
    "    print(f\"  Confidence Threshold : {conf_threshold}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    context_module = CoOpContext(\n",
    "        device=device,\n",
    "        processor=processor,\n",
    "        model=model,\n",
    "        initial_prompt=initial_prompt,\n",
    "    )\n",
    "\n",
    "    print(f\"  Initial Context Vector:\\n {context_module()}\\n\")\n",
    "    print(f\"  Context Vector Shape: {context_module().shape}\\n\")\n",
    "\n",
    "    lr = 5e-4\n",
    "    optimizer = optim.AdamW(context_module.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs,\n",
    "        eta_min=1e-4\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    sup_train_losses = []\n",
    "    unsup_train_losses = []\n",
    "    val_losses = []\n",
    "    train_map50_scores = []\n",
    "    train_map75_scores = []\n",
    "    train_map_scores = []\n",
    "    val_map50_scores = []\n",
    "    val_map75_scores = []\n",
    "    val_map_scores = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    train_map_metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.75]).to(device)\n",
    "    val_map_metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.75]).to(device)\n",
    "    \n",
    "    train_map_metric.warn_on_many_detections = False\n",
    "    val_map_metric.warn_on_many_detections = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"  Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "        if epoch < 5:\n",
    "            current_lambda_u = 0.0\n",
    "        else:\n",
    "            current_lambda_u = lambda_u * min(1.0, (epoch + 1 - 5) / (num_epochs - 5))\n",
    "            \n",
    "        print(f\"  Current Lambda_u: {current_lambda_u:.4f}\\n\")\n",
    "\n",
    "        model.eval()\n",
    "        context_module.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_sup_loss = 0.0\n",
    "        total_unsup_loss = 0.0\n",
    "        sup_batches = 0\n",
    "        unsup_batches = 0\n",
    "        train_map_metric.reset()\n",
    "\n",
    "        sup_iter = iter(sup_train_loader)\n",
    "        unsup_iter = iter(unsup_train_loader)\n",
    "        \n",
    "        max_iters = max(len(sup_train_loader), len(unsup_train_loader))\n",
    "        \n",
    "        train_pbar = tqdm(range(max_iters), desc=\"  Training  \")\n",
    "        \n",
    "        for _ in train_pbar:\n",
    "            use_supervised = np.random.random() > 0.5\n",
    "            \n",
    "            if use_supervised:\n",
    "                try:\n",
    "                    images, annotations = next(sup_iter)\n",
    "                except StopIteration:\n",
    "                    sup_iter = iter(sup_train_loader)\n",
    "                    images, annotations = next(sup_iter)\n",
    "                \n",
    "                batch_size = len(images)\n",
    "                inputs = processor(\n",
    "                    images=images, annotations=annotations, return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                del inputs[\"pixel_mask\"]\n",
    "\n",
    "                context_expanded = context_module().unsqueeze(0).expand(batch_size, -1, -1)\n",
    "                context_token_ids = context_module.initial_token_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "                prompt_enc = {}\n",
    "                prompt_enc[\"input_ids\"] = context_token_ids\n",
    "                prompt_enc[\"inputs_embeds\"] = context_expanded\n",
    "                labels = move_labels_to_device(inputs[\"labels\"], device)\n",
    "                inputs[\"labels\"] = labels\n",
    "\n",
    "                inputs = inputs | prompt_enc\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                loss_dict = outputs.loss_dict\n",
    "                weight_dict = {\n",
    "                    \"loss_ce\": 2.0,\n",
    "                    \"loss_bbox\": model.config.bbox_loss_coefficient,\n",
    "                    \"loss_giou\": model.config.giou_loss_coefficient,\n",
    "                }\n",
    "                enc_weight_dict = {k + \"_enc\": v for k, v in weight_dict.items()}\n",
    "                weight_dict.update(enc_weight_dict)\n",
    "                weight_dict[\"loss_ce_enc\"] = 0\n",
    "                weight_dict[\"loss_bbox_enc\"] = 0\n",
    "                weight_dict[\"loss_giou_enc\"] = 0\n",
    "                loss = sum(\n",
    "                    loss_dict[k] * weight_dict[k]\n",
    "                    for k in loss_dict.keys()\n",
    "                    if k in weight_dict\n",
    "                )\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                total_sup_loss += loss.item()\n",
    "                sup_batches += 1\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    target_sizes = [img.size[::-1] for img in images]\n",
    "                    results = processor.post_process_grounded_object_detection(\n",
    "                        outputs,\n",
    "                        inputs.input_ids,\n",
    "                        threshold=0,\n",
    "                        text_threshold=0,\n",
    "                        target_sizes=target_sizes,\n",
    "                    )\n",
    "\n",
    "                    preds = []\n",
    "                    targets = []\n",
    "\n",
    "                    for res, anno in zip(results, annotations):\n",
    "                        preds.append(\n",
    "                            {\n",
    "                                \"boxes\": res[\"boxes\"],\n",
    "                                \"scores\": res[\"scores\"],\n",
    "                                \"labels\": torch.zeros(\n",
    "                                    len(res[\"boxes\"]), dtype=torch.long, device=device\n",
    "                                ),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        if len(anno[\"annotations\"]) > 0:\n",
    "                            gt_boxes = torch.stack([a[\"bbox\"] for a in anno[\"annotations\"]])\n",
    "                            gt_boxes_xyxy = torch.zeros_like(gt_boxes)\n",
    "                            gt_boxes_xyxy[:, 0] = gt_boxes[:, 0]\n",
    "                            gt_boxes_xyxy[:, 1] = gt_boxes[:, 1]\n",
    "                            gt_boxes_xyxy[:, 2] = gt_boxes[:, 0] + gt_boxes[:, 2]\n",
    "                            gt_boxes_xyxy[:, 3] = gt_boxes[:, 1] + gt_boxes[:, 3]\n",
    "\n",
    "                            targets.append(\n",
    "                                {\n",
    "                                    \"boxes\": gt_boxes_xyxy.to(device),\n",
    "                                    \"labels\": torch.zeros(\n",
    "                                        len(gt_boxes), dtype=torch.long, device=device\n",
    "                                    ),\n",
    "                                }\n",
    "                            )\n",
    "                        else:\n",
    "                            targets.append(\n",
    "                                {\n",
    "                                    \"boxes\": torch.empty((0, 4), device=device),\n",
    "                                    \"labels\": torch.empty(\n",
    "                                        0, dtype=torch.long, device=device\n",
    "                                    ),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                    train_map_metric.update(preds, targets)\n",
    "\n",
    "                train_pbar.set_postfix({\"type\": \"sup\", \"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    images_weak, _ = next(unsup_iter)\n",
    "                except StopIteration:\n",
    "                    unsup_iter = iter(unsup_train_loader)\n",
    "                    images_weak, _ = next(unsup_iter)\n",
    "                \n",
    "                batch_size = len(images_weak)\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    inputs_weak = processor(\n",
    "                        images=images_weak, return_tensors=\"pt\"\n",
    "                    ).to(device)\n",
    "                    del inputs_weak[\"pixel_mask\"]\n",
    "                    \n",
    "                    context_expanded = context_module().unsqueeze(0).expand(batch_size, -1, -1)\n",
    "                    context_token_ids = context_module.initial_token_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "                    \n",
    "                    prompt_enc = {}\n",
    "                    prompt_enc[\"input_ids\"] = context_token_ids\n",
    "                    prompt_enc[\"inputs_embeds\"] = context_expanded\n",
    "                    \n",
    "                    inputs_weak_with_prompt = inputs_weak | prompt_enc\n",
    "                    \n",
    "                    outputs_weak = model(**inputs_weak_with_prompt)\n",
    "                    \n",
    "                    target_sizes = [img.size[::-1] for img in images_weak]\n",
    "                    results_weak = processor.post_process_grounded_object_detection(\n",
    "                        outputs_weak,\n",
    "                        inputs_weak_with_prompt.input_ids,\n",
    "                        threshold=conf_threshold,\n",
    "                        text_threshold=conf_threshold,\n",
    "                        target_sizes=target_sizes,\n",
    "                    )\n",
    "                    \n",
    "                    pseudo_annotations = []\n",
    "                    images_strong = []\n",
    "\n",
    "                    for idx, res in enumerate(results_weak):\n",
    "                        high_conf_mask = res[\"scores\"] > conf_threshold\n",
    "                        pseudo_boxes = res[\"boxes\"][high_conf_mask]\n",
    "                        pseudo_scores = res[\"scores\"][high_conf_mask]\n",
    "                        \n",
    "                        if len(pseudo_boxes) > 0:\n",
    "                            if len(pseudo_boxes) > 5:\n",
    "                                top_k_indices = torch.topk(pseudo_scores, k=5).indices\n",
    "                                pseudo_boxes = pseudo_boxes[top_k_indices]\n",
    "                            \n",
    "                            img_strong, pseudo_boxes_transformed = apply_strong_augmentation(\n",
    "                                images_weak[idx], \n",
    "                                boxes=pseudo_boxes\n",
    "                            )\n",
    "                            \n",
    "                            pseudo_boxes_xywh = torch.zeros_like(pseudo_boxes_transformed)\n",
    "                            pseudo_boxes_xywh[:, 0] = pseudo_boxes_transformed[:, 0]\n",
    "                            pseudo_boxes_xywh[:, 1] = pseudo_boxes_transformed[:, 1]\n",
    "                            pseudo_boxes_xywh[:, 2] = pseudo_boxes_transformed[:, 2] - pseudo_boxes_transformed[:, 0]\n",
    "                            pseudo_boxes_xywh[:, 3] = pseudo_boxes_transformed[:, 3] - pseudo_boxes_transformed[:, 1]\n",
    "                            \n",
    "                            annotations_list = []\n",
    "                            for box in pseudo_boxes_xywh:\n",
    "                                area = float(box[2] * box[3])\n",
    "                                if area > 0:\n",
    "                                    annotations_list.append({\n",
    "                                        \"bbox\": box.cpu().tolist(),\n",
    "                                        \"area\": area,\n",
    "                                        \"category_id\": 0,\n",
    "                                        \"iscrowd\": 0,\n",
    "                                    })\n",
    "                            \n",
    "                            pseudo_annotations.append({\n",
    "                                \"image_id\": idx,\n",
    "                                \"annotations\": annotations_list\n",
    "                            })\n",
    "                        else:\n",
    "                            img_strong = apply_strong_augmentation(images_weak[idx])\n",
    "                            pseudo_annotations.append({\n",
    "                                \"image_id\": idx,\n",
    "                                \"annotations\": []\n",
    "                            })\n",
    "                        \n",
    "                        images_strong.append(img_strong)\n",
    "                \n",
    "                inputs_strong = processor(\n",
    "                    images=images_strong, annotations=pseudo_annotations, return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                del inputs_strong[\"pixel_mask\"]\n",
    "                \n",
    "                context_expanded = context_module().unsqueeze(0).expand(batch_size, -1, -1)\n",
    "                context_token_ids = context_module.initial_token_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "                \n",
    "                prompt_enc = {}\n",
    "                prompt_enc[\"input_ids\"] = context_token_ids\n",
    "                prompt_enc[\"inputs_embeds\"] = context_expanded\n",
    "                labels = move_labels_to_device(inputs_strong[\"labels\"], device)\n",
    "                inputs_strong[\"labels\"] = labels\n",
    "                \n",
    "                inputs_strong = inputs_strong | prompt_enc\n",
    "                \n",
    "                outputs_strong = model(**inputs_strong)\n",
    "                loss_dict = outputs_strong.loss_dict\n",
    "                weight_dict = {\n",
    "                    \"loss_ce\": 2.0,\n",
    "                    \"loss_bbox\": model.config.bbox_loss_coefficient,\n",
    "                    \"loss_giou\": model.config.giou_loss_coefficient,\n",
    "                }\n",
    "                enc_weight_dict = {k + \"_enc\": v for k, v in weight_dict.items()}\n",
    "                weight_dict.update(enc_weight_dict)\n",
    "                weight_dict[\"loss_ce_enc\"] = 0\n",
    "                weight_dict[\"loss_bbox_enc\"] = 0\n",
    "                weight_dict[\"loss_giou_enc\"] = 0\n",
    "                unsup_loss = sum(\n",
    "                    loss_dict[k] * weight_dict[k]\n",
    "                    for k in loss_dict.keys()\n",
    "                    if k in weight_dict\n",
    "                )\n",
    "                \n",
    "                weighted_unsup_loss = current_lambda_u * unsup_loss\n",
    "                \n",
    "                total_train_loss += weighted_unsup_loss.item()\n",
    "                total_unsup_loss += unsup_loss.item()\n",
    "                unsup_batches += 1\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                weighted_unsup_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                num_pseudo_boxes = sum(len(anno[\"annotations\"]) for anno in pseudo_annotations)\n",
    "                train_pbar.set_postfix({\n",
    "                    \"type\": \"unsup\", \n",
    "                    \"loss\": f\"{unsup_loss.item():.4f}\",\n",
    "                })\n",
    "\n",
    "        avg_train_loss = total_train_loss / max_iters\n",
    "        avg_sup_loss = total_sup_loss / sup_batches if sup_batches > 0 else 0\n",
    "        avg_unsup_loss = total_unsup_loss / unsup_batches if unsup_batches > 0 else 0\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        sup_train_losses.append(avg_sup_loss)\n",
    "        unsup_train_losses.append(avg_unsup_loss)\n",
    "\n",
    "        train_metrics = train_map_metric.compute()\n",
    "        train_map50_scores.append(train_metrics[\"map_50\"].item())\n",
    "        train_map75_scores.append(train_metrics[\"map_75\"].item())\n",
    "        train_map_scores.append(train_metrics[\"map\"].item())\n",
    "\n",
    "        print(f\"\\n  Training Metrics:\")\n",
    "        print(f\"    Total Loss        : {avg_train_loss:.6f}\")\n",
    "        print(f\"    Supervised Loss   : {avg_sup_loss:.6f} ({sup_batches} batches)\")\n",
    "        print(f\"    Unsupervised Loss : {avg_unsup_loss:.6f} ({unsup_batches} batches)\")\n",
    "        print(f\"    mAP@50            : {train_metrics['map_50']:.4f}\")\n",
    "        print(f\"    mAP@75            : {train_metrics['map_75']:.4f}\")\n",
    "        print(f\"    mAP (Overall)     : {train_metrics['map']:.4f}\\n\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model.eval()\n",
    "        context_module.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_map_metric.reset()\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=\"  Validation\")\n",
    "        with torch.no_grad():\n",
    "            for images, annotations in val_pbar:\n",
    "                batch_size = len(images)\n",
    "                inputs = processor(\n",
    "                    images=images, annotations=annotations, return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                del inputs[\"pixel_mask\"]\n",
    "\n",
    "                context_expanded = (\n",
    "                    context_module().unsqueeze(0).expand(batch_size, -1, -1)\n",
    "                )\n",
    "                context_token_ids = context_module.initial_token_ids.unsqueeze(\n",
    "                    0\n",
    "                ).expand(batch_size, -1)\n",
    "\n",
    "                prompt_enc = {}\n",
    "                prompt_enc[\"input_ids\"] = context_token_ids\n",
    "                prompt_enc[\"inputs_embeds\"] = context_expanded\n",
    "                labels = move_labels_to_device(inputs[\"labels\"], device)\n",
    "                inputs[\"labels\"] = labels\n",
    "\n",
    "                inputs = inputs | prompt_enc\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                loss_dict = outputs.loss_dict\n",
    "                weight_dict = {\n",
    "                    \"loss_ce\": 2.0,\n",
    "                    \"loss_bbox\": model.config.bbox_loss_coefficient,\n",
    "                    \"loss_giou\": model.config.giou_loss_coefficient,\n",
    "                }\n",
    "                enc_weight_dict = {k + \"_enc\": v for k, v in weight_dict.items()}\n",
    "                weight_dict.update(enc_weight_dict)\n",
    "                weight_dict[\"loss_ce_enc\"] = 0\n",
    "                weight_dict[\"loss_bbox_enc\"] = 0\n",
    "                weight_dict[\"loss_giou_enc\"] = 0\n",
    "                loss = sum(\n",
    "                    loss_dict[k] * weight_dict[k]\n",
    "                    for k in loss_dict.keys()\n",
    "                    if k in weight_dict\n",
    "                )\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                target_sizes = [img.size[::-1] for img in images]\n",
    "                results = processor.post_process_grounded_object_detection(\n",
    "                    outputs,\n",
    "                    inputs.input_ids,\n",
    "                    threshold=0,\n",
    "                    text_threshold=0,\n",
    "                    target_sizes=target_sizes,\n",
    "                )\n",
    "\n",
    "                preds = []\n",
    "                targets = []\n",
    "\n",
    "                for res, anno in zip(results, annotations):\n",
    "                    preds.append(\n",
    "                        {\n",
    "                            \"boxes\": res[\"boxes\"],\n",
    "                            \"scores\": res[\"scores\"],\n",
    "                            \"labels\": torch.zeros(\n",
    "                                len(res[\"boxes\"]), dtype=torch.long, device=device\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    if len(anno[\"annotations\"]) > 0:\n",
    "                        gt_boxes = torch.stack([a[\"bbox\"] for a in anno[\"annotations\"]])\n",
    "                        gt_boxes_xyxy = torch.zeros_like(gt_boxes)\n",
    "                        gt_boxes_xyxy[:, 0] = gt_boxes[:, 0]\n",
    "                        gt_boxes_xyxy[:, 1] = gt_boxes[:, 1]\n",
    "                        gt_boxes_xyxy[:, 2] = gt_boxes[:, 0] + gt_boxes[:, 2]\n",
    "                        gt_boxes_xyxy[:, 3] = gt_boxes[:, 1] + gt_boxes[:, 3]\n",
    "\n",
    "                        targets.append(\n",
    "                            {\n",
    "                                \"boxes\": gt_boxes_xyxy.to(device),\n",
    "                                \"labels\": torch.zeros(\n",
    "                                    len(gt_boxes), dtype=torch.long, device=device\n",
    "                                ),\n",
    "                            }\n",
    "                        )\n",
    "                    else:\n",
    "                        targets.append(\n",
    "                            {\n",
    "                                \"boxes\": torch.empty((0, 4), device=device),\n",
    "                                \"labels\": torch.empty(\n",
    "                                    0, dtype=torch.long, device=device\n",
    "                                ),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                val_map_metric.update(preds, targets)\n",
    "\n",
    "                val_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_metrics = val_map_metric.compute()\n",
    "        val_map50_scores.append(val_metrics[\"map_50\"].item())\n",
    "        val_map75_scores.append(val_metrics[\"map_75\"].item())\n",
    "        val_map_scores.append(val_metrics[\"map\"].item())\n",
    "\n",
    "        print(f\"\\n  Validation Metrics:\")\n",
    "        print(f\"    Loss         : {avg_val_loss:.6f}\")\n",
    "        print(f\"    mAP@50       : {val_metrics['map_50']:.4f}\")\n",
    "        print(f\"    mAP@75       : {val_metrics['map_75']:.4f}\")\n",
    "        print(f\"    mAP (Overall): {val_metrics['map']:.4f}\\n\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"context_vectors\": context_module.all_embeddings.data.clone(),\n",
    "                },\n",
    "                f\"best_context_vectors_{sup_train_name}_{unsup_train_name}.pth\",\n",
    "            )\n",
    "\n",
    "            print(f\"  {'*'*66}\")\n",
    "            print(f\"  *** BEST MODEL SAVED! (Val Loss: {best_val_loss:.6f}) ***\")\n",
    "            print(f\"  {'*'*66}\\n\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Training Complete\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Best Validation Loss: {best_val_loss:.6f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    plot_metrics(\n",
    "        f\"{sup_train_name}_{unsup_train_name}\",\n",
    "        num_epochs,\n",
    "        train_losses,\n",
    "        val_losses,\n",
    "        train_map50_scores,\n",
    "        val_map50_scores,\n",
    "        train_map75_scores,\n",
    "        val_map75_scores,\n",
    "        train_map_scores,\n",
    "        val_map_scores,\n",
    "        sup_train_losses,\n",
    "        unsup_train_losses,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.587695Z",
     "iopub.status.busy": "2025-11-21T12:29:22.587485Z",
     "iopub.status.idle": "2025-11-21T12:29:22.610445Z",
     "shell.execute_reply": "2025-11-21T12:29:22.609592Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.587679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_one(model, processor, context_module, test_loader, dataset_name):\n",
    "    model.eval()\n",
    "    context_module.eval()\n",
    "\n",
    "    test_map_metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.75]).to(device)\n",
    "    test_map_metric.warn_on_many_detections = False\n",
    "    test_map_metric.reset()\n",
    "\n",
    "    total_test_loss = 0.0\n",
    "\n",
    "    test_pbar = tqdm(test_loader, desc=f\"Testing {dataset_name}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, annotations in test_pbar:\n",
    "            batch_size = len(images)\n",
    "\n",
    "            inputs = processor(\n",
    "                images=images, annotations=annotations, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            del inputs[\"pixel_mask\"]\n",
    "\n",
    "            context_expanded = context_module().unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            context_token_ids = context_module.initial_token_ids.unsqueeze(0).expand(\n",
    "                batch_size, -1\n",
    "            )\n",
    "\n",
    "            prompt_enc = {}\n",
    "            prompt_enc[\"input_ids\"] = context_token_ids\n",
    "            prompt_enc[\"inputs_embeds\"] = context_expanded\n",
    "\n",
    "            labels = move_labels_to_device(inputs[\"labels\"], device)\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "            inputs = inputs | prompt_enc\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss_dict = outputs.loss_dict\n",
    "\n",
    "            weight_dict = {\n",
    "                \"loss_ce\": 2.0,\n",
    "                \"loss_bbox\": model.config.bbox_loss_coefficient,\n",
    "                \"loss_giou\": model.config.giou_loss_coefficient,\n",
    "            }\n",
    "            enc_weight_dict = {k + \"_enc\": v for k, v in weight_dict.items()}\n",
    "            weight_dict.update(enc_weight_dict)\n",
    "            weight_dict[\"loss_ce_enc\"] = 0\n",
    "            weight_dict[\"loss_bbox_enc\"] = 0\n",
    "            weight_dict[\"loss_giou_enc\"] = 0\n",
    "\n",
    "            loss = sum(\n",
    "                loss_dict[k] * weight_dict[k]\n",
    "                for k in loss_dict.keys()\n",
    "                if k in weight_dict\n",
    "            )\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            target_sizes = [img.size[::-1] for img in images]\n",
    "            results = processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs.input_ids,\n",
    "                threshold=0,\n",
    "                text_threshold=0,\n",
    "                target_sizes=target_sizes,\n",
    "            )\n",
    "\n",
    "            preds = []\n",
    "            targets = []\n",
    "\n",
    "            for res, anno in zip(results, annotations):\n",
    "                preds.append(\n",
    "                    {\n",
    "                        \"boxes\": res[\"boxes\"],\n",
    "                        \"scores\": res[\"scores\"],\n",
    "                        \"labels\": torch.zeros(\n",
    "                            len(res[\"boxes\"]), dtype=torch.long, device=device\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if len(anno[\"annotations\"]) > 0:\n",
    "                    gt_boxes = torch.stack([a[\"bbox\"] for a in anno[\"annotations\"]])\n",
    "                    gt_boxes_xyxy = torch.zeros_like(gt_boxes)\n",
    "                    gt_boxes_xyxy[:, 0] = gt_boxes[:, 0]\n",
    "                    gt_boxes_xyxy[:, 1] = gt_boxes[:, 1]\n",
    "                    gt_boxes_xyxy[:, 2] = gt_boxes[:, 0] + gt_boxes[:, 2]\n",
    "                    gt_boxes_xyxy[:, 3] = gt_boxes[:, 1] + gt_boxes[:, 3]\n",
    "\n",
    "                    targets.append(\n",
    "                        {\n",
    "                            \"boxes\": gt_boxes_xyxy.to(device),\n",
    "                            \"labels\": torch.zeros(\n",
    "                                len(gt_boxes), dtype=torch.long, device=device\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    targets.append(\n",
    "                        {\n",
    "                            \"boxes\": torch.empty((0, 4), device=device),\n",
    "                            \"labels\": torch.empty(0, dtype=torch.long, device=device),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            test_map_metric.update(preds, targets)\n",
    "            test_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_metrics = test_map_metric.compute()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {dataset_name} Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Average Loss       : {avg_test_loss:.6f}\")\n",
    "    print(f\"  mAP@50            : {test_metrics['map_50']:.4f}\")\n",
    "    print(f\"  mAP@75            : {test_metrics['map_75']:.4f}\")\n",
    "    print(f\"  mAP (Overall)     : {test_metrics['map']:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.611437Z",
     "iopub.status.busy": "2025-11-21T12:29:22.611165Z",
     "iopub.status.idle": "2025-11-21T12:29:22.627182Z",
     "shell.execute_reply": "2025-11-21T12:29:22.626538Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.611411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_all_datasets(model, processor, train_name):\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"#  TESTING MODEL - Trained on Dataset {train_name}\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "\n",
    "    initial_prompt = \"malignant tumor cancer.\"\n",
    "\n",
    "    context_module = CoOpContext(\n",
    "        device=device,\n",
    "        processor=processor,\n",
    "        model=model,\n",
    "        initial_prompt=initial_prompt,\n",
    "    )\n",
    "\n",
    "    def freeze_hook(grad):\n",
    "        mask = context_module.trainable_mask.unsqueeze(-1).to(grad.device)\n",
    "        return grad * mask\n",
    "\n",
    "    context_module.all_embeddings.register_hook(freeze_hook)\n",
    "\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Loading Best Context Vectors from Dataset {train_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        f\"best_context_vectors_{train_name}.pth\", map_location=device\n",
    "    )\n",
    "    context_module.all_embeddings.data = checkpoint[\"context_vectors\"]\n",
    "    print(f\"  âœ“ Context vectors loaded successfully!\")\n",
    "    print(f\"  Context Vector: {context_module.all_embeddings}\")\n",
    "    print(f\"  Shape: {context_module.all_embeddings.shape}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    word_embeddings = model.model.text_backbone.embeddings.word_embeddings.weight\n",
    "    trainable_embeddings = context_module.all_embeddings[context_module.trainable_mask]\n",
    "    context_norm = F.normalize(trainable_embeddings, p=2, dim=1)\n",
    "    word_embeddings_norm = F.normalize(word_embeddings, p=2, dim=1)\n",
    "\n",
    "    similarities = torch.matmul(context_norm, word_embeddings_norm.T)\n",
    "\n",
    "    for i in range(len(trainable_embeddings)):\n",
    "        top_sims, top_indices = torch.topk(similarities[i], k=5)\n",
    "\n",
    "        print(f\"\\nTrainable Context Vector {i}:\")\n",
    "        for sim, idx in zip(top_sims, top_indices):\n",
    "            word = processor.tokenizer.decode([idx.item()])\n",
    "            print(f\"  {word} ({sim.item():.4f})\")\n",
    "\n",
    "    test_loader_1, test_loader_2, test_loader_3 = create_test_dataloaders()\n",
    "\n",
    "    test_one(\n",
    "        model, processor, context_module, test_loader_1, dataset_name=\"Test Dataset A\"\n",
    "    )\n",
    "    test_one(\n",
    "        model, processor, context_module, test_loader_2, dataset_name=\"Test Dataset B\"\n",
    "    )\n",
    "    test_one(\n",
    "        model, processor, context_module, test_loader_3, dataset_name=\"Test Dataset C\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.629673Z",
     "iopub.status.busy": "2025-11-21T12:29:22.629468Z",
     "iopub.status.idle": "2025-11-21T12:29:22.655819Z",
     "shell.execute_reply": "2025-11-21T12:29:22.655202Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.629658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_one(model, processor, context_module, test_loader):\n",
    "    count = 0\n",
    "    max_visualizations = 5\n",
    "\n",
    "    model.eval()\n",
    "    context_module.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, annotations in test_loader:\n",
    "            batch_size = len(images)\n",
    "\n",
    "            inputs = processor(\n",
    "                images=images, annotations=annotations, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            del inputs[\"pixel_mask\"]\n",
    "\n",
    "            context_expanded = context_module().unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            context_token_ids = context_module.initial_token_ids.unsqueeze(0).expand(\n",
    "                batch_size, -1\n",
    "            )\n",
    "\n",
    "            prompt_enc = {}\n",
    "            prompt_enc[\"input_ids\"] = context_token_ids\n",
    "            prompt_enc[\"inputs_embeds\"] = context_expanded\n",
    "\n",
    "            labels = move_labels_to_device(inputs[\"labels\"], device)\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "            inputs = inputs | prompt_enc\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            target_sizes = [img.size[::-1] for img in images]\n",
    "            results = processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs.input_ids,\n",
    "                threshold=0.1,\n",
    "                text_threshold=0.1,\n",
    "                target_sizes=target_sizes,\n",
    "            )\n",
    "\n",
    "            for img, res, lbls in zip(images, results, annotations):\n",
    "                boxes = res[\"boxes\"]\n",
    "                scores = res[\"scores\"]\n",
    "                text_labels = res.get(\"text_labels\", None)\n",
    "\n",
    "                k = 5\n",
    "                top_k = min(k, len(scores))\n",
    "                sorted_indices = scores.argsort(descending=True)[:top_k]\n",
    "                boxes = boxes[sorted_indices]\n",
    "                scores = scores[sorted_indices]\n",
    "                if text_labels is not None:\n",
    "                    text_labels = [text_labels[i] for i in sorted_indices.cpu().numpy()]\n",
    "\n",
    "                fig, (ax_img, ax_legend) = plt.subplots(\n",
    "                    1, 2, figsize=(16, 10), gridspec_kw={\"width_ratios\": [5, 1]}\n",
    "                )\n",
    "\n",
    "                ax_img.imshow(img)\n",
    "                ax_img.set_title(\n",
    "                    f\"Predictions: {len(boxes)} | GT: {len(lbls.get('annotations', []))}\",\n",
    "                    fontsize=14,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "                ax_img.axis(\"off\")\n",
    "\n",
    "                if \"annotations\" in lbls and len(lbls[\"annotations\"]) > 0:\n",
    "                    for ann in lbls[\"annotations\"]:\n",
    "                        bbox = ann[\"bbox\"].cpu().numpy()\n",
    "                        rect = patches.Rectangle(\n",
    "                            (bbox[0], bbox[1]),\n",
    "                            bbox[2],\n",
    "                            bbox[3],\n",
    "                            linewidth=3,\n",
    "                            edgecolor=\"red\",\n",
    "                            facecolor=\"none\",\n",
    "                        )\n",
    "                        ax_img.add_patch(rect)\n",
    "\n",
    "                        ax_img.text(\n",
    "                            bbox[0],\n",
    "                            bbox[1] - 5,\n",
    "                            \"GT\",\n",
    "                            color=\"white\",\n",
    "                            fontsize=10,\n",
    "                            fontweight=\"bold\",\n",
    "                            bbox=dict(\n",
    "                                facecolor=\"red\", alpha=0.9, edgecolor=\"none\", pad=1\n",
    "                            ),\n",
    "                        )\n",
    "\n",
    "                legend_text = []\n",
    "                for idx, (pred, score) in enumerate(zip(boxes, scores)):\n",
    "                    x1, y1, x2, y2 = pred.cpu().numpy()\n",
    "                    score_val = score.cpu().item()\n",
    "\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1),\n",
    "                        x2 - x1,\n",
    "                        y2 - y1,\n",
    "                        linewidth=3,\n",
    "                        edgecolor=\"blue\",\n",
    "                        facecolor=\"none\",\n",
    "                    )\n",
    "                    ax_img.add_patch(rect)\n",
    "\n",
    "                    ax_img.text(\n",
    "                        x1,\n",
    "                        y1 - 5,\n",
    "                        str(idx + 1),\n",
    "                        color=\"white\",\n",
    "                        fontsize=14,\n",
    "                        fontweight=\"bold\",\n",
    "                        bbox=dict(facecolor=\"blue\", alpha=0.9, edgecolor=\"none\", pad=2),\n",
    "                    )\n",
    "\n",
    "                    if text_labels is not None and idx < len(text_labels):\n",
    "                        label_str = text_labels[idx]\n",
    "                    else:\n",
    "                        label_str = \"N/A\"\n",
    "\n",
    "                    legend_text.append(\n",
    "                        f\"{idx + 1}. Score: {score_val:.3f}\\n   Label: {label_str}\"\n",
    "                    )\n",
    "\n",
    "                ax_legend.axis(\"off\")\n",
    "                ax_legend.set_xlim(0, 1)\n",
    "                ax_legend.set_ylim(0, 1)\n",
    "\n",
    "                ax_legend.text(\n",
    "                    0.05,\n",
    "                    0.95,\n",
    "                    \"Predictions\",\n",
    "                    fontsize=16,\n",
    "                    fontweight=\"bold\",\n",
    "                    verticalalignment=\"top\",\n",
    "                )\n",
    "\n",
    "                y_pos = 0.88\n",
    "                for text in legend_text:\n",
    "                    ax_legend.text(\n",
    "                        0.05,\n",
    "                        y_pos,\n",
    "                        text,\n",
    "                        fontsize=11,\n",
    "                        verticalalignment=\"top\",\n",
    "                        family=\"monospace\",\n",
    "                    )\n",
    "                    y_pos -= 0.12\n",
    "\n",
    "                if \"annotations\" in lbls and len(lbls[\"annotations\"]) > 0:\n",
    "                    gt_y_pos = y_pos - 0.05\n",
    "                    ax_legend.text(\n",
    "                        0.05,\n",
    "                        gt_y_pos,\n",
    "                        \"Ground Truth\",\n",
    "                        fontsize=14,\n",
    "                        fontweight=\"bold\",\n",
    "                        verticalalignment=\"top\",\n",
    "                        color=\"red\",\n",
    "                    )\n",
    "                    ax_legend.text(\n",
    "                        0.05,\n",
    "                        gt_y_pos - 0.08,\n",
    "                        f\"GT. {len(lbls['annotations'])} lesion(s)\",\n",
    "                        fontsize=11,\n",
    "                        verticalalignment=\"top\",\n",
    "                        family=\"monospace\",\n",
    "                        color=\"red\",\n",
    "                    )\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            count += 1\n",
    "            if count >= max_visualizations:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.656799Z",
     "iopub.status.busy": "2025-11-21T12:29:22.656593Z",
     "iopub.status.idle": "2025-11-21T12:29:22.674771Z",
     "shell.execute_reply": "2025-11-21T12:29:22.674015Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.656783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_all_datasets(model, processor, train_name):\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"#  Visualize MODEL - Trained on Dataset {train_name}\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "\n",
    "    initial_prompt = \"malignant tumor cancer.\"\n",
    "\n",
    "    context_module = CoOpContext(\n",
    "        device=device,\n",
    "        processor=processor,\n",
    "        model=model,\n",
    "        initial_prompt=initial_prompt,\n",
    "    )\n",
    "\n",
    "    def freeze_hook(grad):\n",
    "        mask = context_module.trainable_mask.unsqueeze(-1).to(grad.device)\n",
    "        return grad * mask\n",
    "\n",
    "    context_module.all_embeddings.register_hook(freeze_hook)\n",
    "\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Loading Best Context Vectors from Dataset {train_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        f\"best_context_vectors_{train_name}.pth\", map_location=device\n",
    "    )\n",
    "    context_module.all_embeddings.data = checkpoint[\"context_vectors\"]\n",
    "    print(f\"  âœ“ Context vectors loaded successfully!\")\n",
    "\n",
    "    test_loader_1, test_loader_2, test_loader_3 = create_test_dataloaders()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Visualizing Test Dataset A\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    visualize_one(model, processor, context_module, test_loader_1)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Visualizing Test Dataset B\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    visualize_one(model, processor, context_module, test_loader_2)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Visualizing Test Dataset C\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    visualize_one(model, processor, context_module, test_loader_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:22.675718Z",
     "iopub.status.busy": "2025-11-21T12:29:22.675483Z",
     "iopub.status.idle": "2025-11-21T12:29:27.265210Z",
     "shell.execute_reply": "2025-11-21T12:29:27.264511Z",
     "shell.execute_reply.started": "2025-11-21T12:29:22.675701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"#  INITIALIZING MODEL\")\n",
    "print(f\"{'#'*70}\\n\")\n",
    "print(f\"  Model: {model_id}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "print(f\"  Model loaded and frozen successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T12:29:27.266124Z",
     "iopub.status.busy": "2025-11-21T12:29:27.265903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_one_dataset(model=model, processor=processor, sup_train_name=\"A\", unsup_train_name=\"B\", num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_all_datasets(model=model, processor=processor, train_name=\"A_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_all_datasets(model=model, processor=processor, train_name=\"A_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_one_dataset(model=model, processor=processor, sup_train_name=\"A\", unsup_train_name=\"C\", num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_all_datasets(model=model, processor=processor, train_name=\"A_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_all_datasets(model=model, processor=processor, train_name=\"A_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8550063,
     "sourceId": 13468902,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8661278,
     "sourceId": 13627453,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
